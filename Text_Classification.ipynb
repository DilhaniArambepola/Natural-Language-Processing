{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "NLTK: 3.4.5\n",
      "Scikit-learn: 0.22.1\n",
      "Pandas: 1.0.1\n",
      "Numpy: 1.18.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pandas.__version__))\n",
    "print('Numpy: {}'.format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Load the Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('SMSSpamCollection', header = None, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       5572 non-null   object\n",
      " 1   1       5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4825\n",
      "spam     747\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classes = df[0]\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Preprocess the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     ham\n",
      "1     ham\n",
      "2    spam\n",
      "3     ham\n",
      "4     ham\n",
      "5    spam\n",
      "6     ham\n",
      "7     ham\n",
      "8    spam\n",
      "9    spam\n",
      "Name: 0, dtype: object\n",
      "[0 0 1 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Convert class labels to binary values, 0 = ham, 1 = spam\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)\n",
    "\n",
    "print(classes[:10])\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "5    FreeMsg Hey there darling it's been 3 week's n...\n",
      "6    Even my brother is not like to speak with me. ...\n",
      "7    As per your request 'Melle Melle (Oru Minnamin...\n",
      "8    WINNER!! As a valued network customer you have...\n",
      "9    Had your mobile 11 months or more? U R entitle...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#store the SMS message data\n",
    "text_messages = df[1]\n",
    "print(text_messages[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expressions to replace email adresses, urls, phone numbors and symbols\n",
    "\n",
    "# replace email addresses with 'emailaddr'\n",
    "processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$', 'emailaddr')\n",
    "\n",
    "# replace urls with 'webaddress'\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\,]+\\.[a-zA-Z]{2,3}(/\\$*)?$', 'webaddress')\n",
    "\n",
    "#  replace money symbols with 'moneysymb'\n",
    "processed = processed.str.replace(r'£|\\$', 'moneysymb')\n",
    "\n",
    "# replace 10 digit phone numbers with 'phonenumber'\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}[-\\.\\s]\\d{3}[-\\.\\s]\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]\\d{4}|\\d{3}[-\\.\\s]\\d{4}$', 'phonenumber')\n",
    "\n",
    "# replace normal numbers with 'number'\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "# replace whitespace between terms with a single space\n",
    "processed = processed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go until jurong point crazy available only in ...\n",
      "1                                 ok lar joking wif u oni\n",
      "2       free entry in number a wkly comp to win fa cup...\n",
      "3             u dun say so early hor u c already then say\n",
      "4       nah i don t think he goes to usf he lives arou...\n",
      "                              ...                        \n",
      "5567    this is the numbernd time we have tried number...\n",
      "5568                  will ü b going to esplanade fr home\n",
      "5569    pity was in mood for that so any other suggest...\n",
      "5570    the guy did some bitching but i acted like i d...\n",
      "5571                            rofl its true to its name\n",
      "Name: 1, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# change words to lowercase\n",
    "processed = processed.str.lower()\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words from text messages\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = processed.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove word stems using a Porter stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go until jurong point crazi avail onli in bugi...\n",
      "1                                   ok lar joke wif u oni\n",
      "2       free entri in number a wkli comp to win fa cup...\n",
      "3             u dun say so earli hor u c alreadi then say\n",
      "4       nah i don t think he goe to usf he live around...\n",
      "                              ...                        \n",
      "5567    thi is the numbernd time we have tri number co...\n",
      "5568                      will ü b go to esplanad fr home\n",
      "5569        piti wa in mood for that so ani other suggest\n",
      "5570    the guy did some bitch but i act like i d be i...\n",
      "5571                              rofl it true to it name\n",
      "Name: 1, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nimasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a bag-of-words\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 6680\n",
      "Most common words: [('i', 3020), ('number', 2757), ('to', 2251), ('you', 2245), ('a', 1448), ('the', 1339), ('u', 1207), ('and', 979), ('it', 978), ('in', 903), ('is', 896), ('me', 807), ('my', 767), ('for', 709), ('your', 705)]\n"
     ]
    }
   ],
   "source": [
    "# print the local number words and the 15 most common words\n",
    "print('Number of words: {}'.format(len(all_words)))\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 1500 most common words as features\n",
    "word_features = list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazi',\n",
       " 'avail',\n",
       " 'onli',\n",
       " 'in',\n",
       " 'bugi',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amor',\n",
       " 'wat',\n",
       " 'ok',\n",
       " 'lar',\n",
       " 'joke',\n",
       " 'wif',\n",
       " 'u',\n",
       " 'oni',\n",
       " 'free',\n",
       " 'entri',\n",
       " 'number',\n",
       " 'a',\n",
       " 'wkli',\n",
       " 'comp',\n",
       " 'to',\n",
       " 'win',\n",
       " 'fa',\n",
       " 'cup',\n",
       " 'final',\n",
       " 'tkt',\n",
       " 'numberst',\n",
       " 'may',\n",
       " 'text',\n",
       " 'receiv',\n",
       " 'question',\n",
       " 'std',\n",
       " 'txt',\n",
       " 'rate',\n",
       " 't',\n",
       " 'c',\n",
       " 's',\n",
       " 'appli',\n",
       " 'numberovernumb',\n",
       " 'dun',\n",
       " 'say',\n",
       " 'so',\n",
       " 'earli',\n",
       " 'hor',\n",
       " 'alreadi',\n",
       " 'then',\n",
       " 'nah',\n",
       " 'i',\n",
       " 'don',\n",
       " 'think',\n",
       " 'he',\n",
       " 'goe',\n",
       " 'usf',\n",
       " 'live',\n",
       " 'around',\n",
       " 'here',\n",
       " 'though',\n",
       " 'freemsg',\n",
       " 'hey',\n",
       " 'darl',\n",
       " 'it',\n",
       " 'been',\n",
       " 'week',\n",
       " 'now',\n",
       " 'and',\n",
       " 'no',\n",
       " 'word',\n",
       " 'back',\n",
       " 'd',\n",
       " 'like',\n",
       " 'some',\n",
       " 'fun',\n",
       " 'you',\n",
       " 'up',\n",
       " 'for',\n",
       " 'still',\n",
       " 'tb',\n",
       " 'xxx',\n",
       " 'chg',\n",
       " 'send',\n",
       " 'moneysymbnumb',\n",
       " 'rcv',\n",
       " 'even',\n",
       " 'my',\n",
       " 'brother',\n",
       " 'is',\n",
       " 'not',\n",
       " 'speak',\n",
       " 'with',\n",
       " 'me',\n",
       " 'they',\n",
       " 'treat',\n",
       " 'aid',\n",
       " 'patent',\n",
       " 'as',\n",
       " 'per',\n",
       " 'your',\n",
       " 'request',\n",
       " 'mell',\n",
       " 'oru',\n",
       " 'minnaminungint',\n",
       " 'nurungu',\n",
       " 'vettam',\n",
       " 'ha',\n",
       " 'set',\n",
       " 'callertun',\n",
       " 'all',\n",
       " 'caller',\n",
       " 'press',\n",
       " 'copi',\n",
       " 'friend',\n",
       " 'winner',\n",
       " 'valu',\n",
       " 'network',\n",
       " 'custom',\n",
       " 'have',\n",
       " 'select',\n",
       " 'receivea',\n",
       " 'prize',\n",
       " 'reward',\n",
       " 'claim',\n",
       " 'call',\n",
       " 'code',\n",
       " 'klnumber',\n",
       " 'valid',\n",
       " 'hour',\n",
       " 'had',\n",
       " 'mobil',\n",
       " 'month',\n",
       " 'or',\n",
       " 'more',\n",
       " 'r',\n",
       " 'entitl',\n",
       " 'updat',\n",
       " 'the',\n",
       " 'latest',\n",
       " 'colour',\n",
       " 'camera',\n",
       " 'co',\n",
       " 'on',\n",
       " 'm',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'be',\n",
       " 'home',\n",
       " 'soon',\n",
       " 'want',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'thi',\n",
       " 'stuff',\n",
       " 'anymor',\n",
       " 'tonight',\n",
       " 'k',\n",
       " 've',\n",
       " 'cri',\n",
       " 'enough',\n",
       " 'today',\n",
       " 'six',\n",
       " 'chanc',\n",
       " 'cash',\n",
       " 'from',\n",
       " 'pound',\n",
       " 'cshnumber',\n",
       " 'cost',\n",
       " 'numberp',\n",
       " 'day',\n",
       " 'numberday',\n",
       " 'tsandc',\n",
       " 'repli',\n",
       " 'hl',\n",
       " 'info',\n",
       " 'urgent',\n",
       " 'won',\n",
       " 'membership',\n",
       " 'our',\n",
       " 'jackpot',\n",
       " 'www',\n",
       " 'dbuk',\n",
       " 'net',\n",
       " 'lccltd',\n",
       " 'pobox',\n",
       " 'numberldnwnumberanumberrwnumb',\n",
       " 'search',\n",
       " 'right',\n",
       " 'thank',\n",
       " 'breather',\n",
       " 'promis',\n",
       " 'wont',\n",
       " 'take',\n",
       " 'help',\n",
       " 'grant',\n",
       " 'will',\n",
       " 'fulfil',\n",
       " 'wonder',\n",
       " 'bless',\n",
       " 'at',\n",
       " 'time',\n",
       " 'date',\n",
       " 'sunday',\n",
       " 'xxxmobilemovieclub',\n",
       " 'use',\n",
       " 'credit',\n",
       " 'click',\n",
       " 'wap',\n",
       " 'link',\n",
       " 'next',\n",
       " 'messag',\n",
       " 'http',\n",
       " 'com',\n",
       " 'qjkgighjjgcbl',\n",
       " 'oh',\n",
       " 'watch',\n",
       " 'eh',\n",
       " 'rememb',\n",
       " 'how',\n",
       " 'spell',\n",
       " 'hi',\n",
       " 'name',\n",
       " 'ye',\n",
       " 'did',\n",
       " 'v',\n",
       " 'naughti',\n",
       " 'make',\n",
       " 'wet',\n",
       " 'fine',\n",
       " 'if',\n",
       " 'that',\n",
       " 'way',\n",
       " 'feel',\n",
       " 'gota',\n",
       " 'b',\n",
       " 'england',\n",
       " 'macedonia',\n",
       " 'dont',\n",
       " 'miss',\n",
       " 'goal',\n",
       " 'team',\n",
       " 'news',\n",
       " 'ur',\n",
       " 'nation',\n",
       " 'eg',\n",
       " 'tri',\n",
       " 'wale',\n",
       " 'scotland',\n",
       " 'numbertxt',\n",
       " 'únumber',\n",
       " 'poboxoxnumberwnumberwq',\n",
       " 'serious',\n",
       " 'ü',\n",
       " 'pay',\n",
       " 'first',\n",
       " 'when',\n",
       " 'da',\n",
       " 'stock',\n",
       " 'comin',\n",
       " 'aft',\n",
       " 'finish',\n",
       " 'lunch',\n",
       " 'str',\n",
       " 'down',\n",
       " 'lor',\n",
       " 'ard',\n",
       " 'smth',\n",
       " 'ffffffffff',\n",
       " 'alright',\n",
       " 'can',\n",
       " 'meet',\n",
       " 'sooner',\n",
       " 'just',\n",
       " 'forc',\n",
       " 'myself',\n",
       " 'eat',\n",
       " 'slice',\n",
       " 'realli',\n",
       " 'hungri',\n",
       " 'tho',\n",
       " 'suck',\n",
       " 'mark',\n",
       " 'get',\n",
       " 'worri',\n",
       " 'know',\n",
       " 'sick',\n",
       " 'turn',\n",
       " 'pizza',\n",
       " 'lol',\n",
       " 'alway',\n",
       " 'convinc',\n",
       " 'catch',\n",
       " 'bu',\n",
       " 'are',\n",
       " 'fri',\n",
       " 'an',\n",
       " 'egg',\n",
       " 'tea',\n",
       " 'mom',\n",
       " 'left',\n",
       " 'over',\n",
       " 'dinner',\n",
       " 'do',\n",
       " 'love',\n",
       " 'amp',\n",
       " 'we',\n",
       " 're',\n",
       " 'pack',\n",
       " 'car',\n",
       " 'll',\n",
       " 'let',\n",
       " 'room',\n",
       " 'ahhh',\n",
       " 'work',\n",
       " 'vagu',\n",
       " 'what',\n",
       " 'doe',\n",
       " 'wait',\n",
       " 'clear',\n",
       " 'were',\n",
       " 'sure',\n",
       " 'sarcast',\n",
       " 'whi',\n",
       " 'x',\n",
       " 'doesn',\n",
       " 'us',\n",
       " 'yeah',\n",
       " 'wa',\n",
       " 'apologet',\n",
       " 'fallen',\n",
       " 'out',\n",
       " 'she',\n",
       " 'actin',\n",
       " 'spoilt',\n",
       " 'child',\n",
       " 'caught',\n",
       " 'till',\n",
       " 'but',\n",
       " 'too',\n",
       " 'badli',\n",
       " 'cheer',\n",
       " 'tell',\n",
       " 'anyth',\n",
       " 'fear',\n",
       " 'of',\n",
       " 'faint',\n",
       " 'housework',\n",
       " 'quick',\n",
       " 'cuppa',\n",
       " 'subscript',\n",
       " 'rington',\n",
       " 'uk',\n",
       " 'charg',\n",
       " 'pleas',\n",
       " 'confirm',\n",
       " 'by',\n",
       " 'yup',\n",
       " 'look',\n",
       " 'msg',\n",
       " 'again',\n",
       " 'xuhui',\n",
       " 'learn',\n",
       " 'numbernd',\n",
       " 'her',\n",
       " 'lesson',\n",
       " 'numberam',\n",
       " 'oop',\n",
       " 'roommat',\n",
       " 'done',\n",
       " 'see',\n",
       " 'letter',\n",
       " 'decid',\n",
       " 'hello',\n",
       " 'saturday',\n",
       " 'tomo',\n",
       " 'invit',\n",
       " 'pl',\n",
       " 'ahead',\n",
       " 'watt',\n",
       " 'weekend',\n",
       " 'abiola',\n",
       " 'forget',\n",
       " 'need',\n",
       " 'crave',\n",
       " 'most',\n",
       " 'sweet',\n",
       " 'arabian',\n",
       " 'steed',\n",
       " 'mmmmmm',\n",
       " 'yummi',\n",
       " 'rodger',\n",
       " 'burn',\n",
       " 'sm',\n",
       " 'nokia',\n",
       " 'camcord',\n",
       " 'deliveri',\n",
       " 'tomorrow',\n",
       " 'who',\n",
       " 'hope',\n",
       " 'man',\n",
       " 'well',\n",
       " 'endow',\n",
       " 'am',\n",
       " 'lt',\n",
       " 'gt',\n",
       " 'inch',\n",
       " 'didn',\n",
       " 'hep',\n",
       " 'immunis',\n",
       " 'nigeria',\n",
       " 'fair',\n",
       " 'tyler',\n",
       " 'could',\n",
       " 'mayb',\n",
       " 'ask',\n",
       " 'bit',\n",
       " 'stubborn',\n",
       " 'hospit',\n",
       " 'kept',\n",
       " 'weak',\n",
       " 'sucker',\n",
       " 'saw',\n",
       " 'class',\n",
       " 'gram',\n",
       " 'usual',\n",
       " 'run',\n",
       " 'half',\n",
       " 'eighth',\n",
       " 'smarter',\n",
       " 'almost',\n",
       " 'whole',\n",
       " 'second',\n",
       " 'fyi',\n",
       " 'ride',\n",
       " 'morn',\n",
       " 'crash',\n",
       " 'place',\n",
       " 'wow',\n",
       " 'never',\n",
       " 'realiz',\n",
       " 'embarass',\n",
       " 'accomod',\n",
       " 'thought',\n",
       " 'sinc',\n",
       " 'best',\n",
       " 'seem',\n",
       " 'happi',\n",
       " 'cave',\n",
       " 'sorri',\n",
       " 'give',\n",
       " 'offer',\n",
       " 'ac',\n",
       " 'sptv',\n",
       " 'new',\n",
       " 'jersey',\n",
       " 'devil',\n",
       " 'detroit',\n",
       " 'red',\n",
       " 'wing',\n",
       " 'play',\n",
       " 'ice',\n",
       " 'hockey',\n",
       " 'correct',\n",
       " 'incorrect',\n",
       " 'end',\n",
       " 'mallika',\n",
       " 'sherawat',\n",
       " 'yesterday',\n",
       " 'find',\n",
       " 'url',\n",
       " 'congrat',\n",
       " 'year',\n",
       " 'special',\n",
       " 'cinema',\n",
       " 'pass',\n",
       " 'suprman',\n",
       " 'matrixnumb',\n",
       " 'starwarsnumb',\n",
       " 'etc',\n",
       " 'bxnumber',\n",
       " 'ipnumb',\n",
       " 'numberw',\n",
       " 'numberpm',\n",
       " 'later',\n",
       " 'where',\n",
       " 'reach',\n",
       " 'gauti',\n",
       " 'sehwag',\n",
       " 'odi',\n",
       " 'seri',\n",
       " 'pick',\n",
       " 'burger',\n",
       " 'yourself',\n",
       " 'move',\n",
       " 'pain',\n",
       " 'kill',\n",
       " 'good',\n",
       " 'girl',\n",
       " 'situat',\n",
       " 'seeker',\n",
       " 'part',\n",
       " 'check',\n",
       " 'iq',\n",
       " 'took',\n",
       " 'forev',\n",
       " 'come',\n",
       " 'doubl',\n",
       " 'hair',\n",
       " 'dresser',\n",
       " 'said',\n",
       " 'wun',\n",
       " 'cut',\n",
       " 'short',\n",
       " 'nice',\n",
       " 'advis',\n",
       " 'follow',\n",
       " 'recent',\n",
       " 'review',\n",
       " 'mob',\n",
       " 'award',\n",
       " 'bonu',\n",
       " 'song',\n",
       " 'dedic',\n",
       " 'which',\n",
       " 'valuabl',\n",
       " 'frnd',\n",
       " 'rpli',\n",
       " 'complimentari',\n",
       " 'trip',\n",
       " 'eurodisinc',\n",
       " 'trav',\n",
       " 'aco',\n",
       " 'entrynumb',\n",
       " 'di',\n",
       " 'morefrmmob',\n",
       " 'shracomorsglsuplt',\n",
       " 'lsnumber',\n",
       " 'numberaj',\n",
       " 'hear',\n",
       " 'divorc',\n",
       " 'barbi',\n",
       " 'ken',\n",
       " 'plane',\n",
       " 'wah',\n",
       " 'lucki',\n",
       " 'save',\n",
       " 'money',\n",
       " 'hee',\n",
       " 'babe',\n",
       " 'im',\n",
       " 'wan',\n",
       " 'someth',\n",
       " 'xx',\n",
       " 'perform',\n",
       " 'machan',\n",
       " 'onc',\n",
       " 'cool',\n",
       " 'gentleman',\n",
       " 'digniti',\n",
       " 'respect',\n",
       " 'peopl',\n",
       " 'veri',\n",
       " 'much',\n",
       " 'shi',\n",
       " 'pa',\n",
       " 'oper',\n",
       " 'after',\n",
       " 'same',\n",
       " 'job',\n",
       " 'ta',\n",
       " 'earn',\n",
       " 'ah',\n",
       " 'stop',\n",
       " 'urgnt',\n",
       " 'real',\n",
       " 'yo',\n",
       " 'ticket',\n",
       " 'one',\n",
       " 'jacket',\n",
       " 'multi',\n",
       " 'start',\n",
       " 'came',\n",
       " 'bed',\n",
       " 'coin',\n",
       " 'factori',\n",
       " 'nitro',\n",
       " 'ela',\n",
       " 'kano',\n",
       " 'il',\n",
       " 'download',\n",
       " 'wen',\n",
       " 'stand',\n",
       " 'close',\n",
       " 'anoth',\n",
       " 'night',\n",
       " 'spent',\n",
       " 'late',\n",
       " 'afternoon',\n",
       " 'casualti',\n",
       " 'mean',\n",
       " 'haven',\n",
       " 'ani',\n",
       " 'y',\n",
       " 'stuffnumbermoro',\n",
       " 'includ',\n",
       " 'sheet',\n",
       " 'smile',\n",
       " 'pleasur',\n",
       " 'troubl',\n",
       " 'pour',\n",
       " 'rain',\n",
       " 'sumnumb',\n",
       " 'hurt',\n",
       " 'becoz',\n",
       " 'someon',\n",
       " 'servic',\n",
       " 'repres',\n",
       " 'between',\n",
       " 'guarante',\n",
       " 'havent',\n",
       " 'plan',\n",
       " 'buy',\n",
       " 'lido',\n",
       " 'show',\n",
       " 'collect',\n",
       " 'simpli',\n",
       " 'password',\n",
       " 'mix',\n",
       " 'verifi',\n",
       " 'usher',\n",
       " 'britney',\n",
       " 'fml',\n",
       " 'po',\n",
       " 'box',\n",
       " 'mknumber',\n",
       " 'numberh',\n",
       " 'numberppw',\n",
       " 'telugu',\n",
       " 'movi',\n",
       " 'abt',\n",
       " 'load',\n",
       " 'loan',\n",
       " 'wk',\n",
       " 'hol',\n",
       " 'forgot',\n",
       " 'hairdress',\n",
       " 'appoint',\n",
       " 'four',\n",
       " 'shower',\n",
       " 'beforehand',\n",
       " 'caus',\n",
       " 'prob',\n",
       " 'coffe',\n",
       " 'anim',\n",
       " 'noth',\n",
       " 'els',\n",
       " 'okay',\n",
       " 'price',\n",
       " 'long',\n",
       " 'legal',\n",
       " 'them',\n",
       " 'ave',\n",
       " 'gone',\n",
       " 'numberth',\n",
       " 'drive',\n",
       " 'test',\n",
       " 'yet',\n",
       " 'guess',\n",
       " 'gave',\n",
       " 'boston',\n",
       " 'men',\n",
       " 'chang',\n",
       " 'locat',\n",
       " 'nyc',\n",
       " 'cuz',\n",
       " 'signin',\n",
       " 'page',\n",
       " 'umma',\n",
       " 'life',\n",
       " 'vava',\n",
       " 'lot',\n",
       " 'dear',\n",
       " 'wish',\n",
       " 'birthday',\n",
       " 'truli',\n",
       " 'memor',\n",
       " 'aight',\n",
       " 'hit',\n",
       " 'would',\n",
       " 'ip',\n",
       " 'address',\n",
       " 'consid',\n",
       " 'comput',\n",
       " 'isn',\n",
       " 'minecraft',\n",
       " 'server',\n",
       " 'grumpi',\n",
       " 'old',\n",
       " 'better',\n",
       " 'lie',\n",
       " 'busi',\n",
       " 'plural',\n",
       " 'noun',\n",
       " 'research',\n",
       " 'thing',\n",
       " 'scare',\n",
       " 'mah',\n",
       " 'loud',\n",
       " 'gent',\n",
       " 'contact',\n",
       " 'last',\n",
       " 'draw',\n",
       " 'knumber',\n",
       " 'numberhr',\n",
       " 'numberppm',\n",
       " 'openin',\n",
       " 'sentenc',\n",
       " 'formal',\n",
       " 'anyway',\n",
       " 'juz',\n",
       " 'tt',\n",
       " 'eatin',\n",
       " 'puttin',\n",
       " 'weight',\n",
       " 'haha',\n",
       " 'anythin',\n",
       " 'happen',\n",
       " 'enter',\n",
       " 'cabin',\n",
       " 'boss',\n",
       " 'felt',\n",
       " 'askd',\n",
       " 'apart',\n",
       " 'went',\n",
       " 'holiday',\n",
       " 'flight',\n",
       " 'inc',\n",
       " 'min',\n",
       " 'goodo',\n",
       " 'must',\n",
       " 'friday',\n",
       " 'potato',\n",
       " 'ratio',\n",
       " 'tortilla',\n",
       " 'hmm',\n",
       " 'uncl',\n",
       " 'inform',\n",
       " 'school',\n",
       " 'directli',\n",
       " 'food',\n",
       " 'privat',\n",
       " 'account',\n",
       " 'statement',\n",
       " 'unredeem',\n",
       " 'identifi',\n",
       " 'expir',\n",
       " 'landlin',\n",
       " 'boxnumberwrnumberc',\n",
       " 'appl',\n",
       " 'pair',\n",
       " 'malarki',\n",
       " 'voda',\n",
       " 'match',\n",
       " 'quot',\n",
       " 'standard',\n",
       " 'app',\n",
       " 'sao',\n",
       " 'mu',\n",
       " 'predict',\n",
       " 'yetund',\n",
       " 'hasn',\n",
       " 'sent',\n",
       " 'bother',\n",
       " 'involv',\n",
       " 'shouldn',\n",
       " 'impos',\n",
       " 'apologis',\n",
       " 'del',\n",
       " 'bak',\n",
       " 'sum',\n",
       " 'lucyxx',\n",
       " 'tmorrow',\n",
       " 'answer',\n",
       " 'sunshin',\n",
       " 'quiz',\n",
       " 'q',\n",
       " 'top',\n",
       " 'soni',\n",
       " 'dvd',\n",
       " 'player',\n",
       " 'countri',\n",
       " 'algarv',\n",
       " 'ansr',\n",
       " 'sp',\n",
       " 'tyron',\n",
       " 'laid',\n",
       " 'dog',\n",
       " 'direct',\n",
       " 'join',\n",
       " 'largest',\n",
       " 'bt',\n",
       " 'txting',\n",
       " 'gravel',\n",
       " 'nt',\n",
       " 'ecnumbera',\n",
       " 'emailaddr',\n",
       " 'him',\n",
       " 'befor',\n",
       " 'activ',\n",
       " 'chat',\n",
       " 'svc',\n",
       " 'hardcor',\n",
       " 'age',\n",
       " 'yr',\n",
       " 'lazi',\n",
       " 'type',\n",
       " 'lect',\n",
       " 'pouch',\n",
       " 'sir',\n",
       " 'mail',\n",
       " 'swt',\n",
       " 'nver',\n",
       " 'tire',\n",
       " 'littl',\n",
       " 'lovabl',\n",
       " 'person',\n",
       " 'coz',\n",
       " 'somtim',\n",
       " 'those',\n",
       " 'occupi',\n",
       " 'biggest',\n",
       " 'their',\n",
       " 'heart',\n",
       " 'gud',\n",
       " 'ninumb',\n",
       " 'open',\n",
       " 'ya',\n",
       " 'dot',\n",
       " 'staff',\n",
       " 'randi',\n",
       " 'sexi',\n",
       " 'femal',\n",
       " 'local',\n",
       " 'luv',\n",
       " 'netcollex',\n",
       " 'ltd',\n",
       " 'ummma',\n",
       " 'begin',\n",
       " 'qatar',\n",
       " 'pray',\n",
       " 'hard',\n",
       " 'delet',\n",
       " 'sindu',\n",
       " 'birla',\n",
       " 'soft',\n",
       " 'wine',\n",
       " 'flow',\n",
       " 'thk',\n",
       " 'plaza',\n",
       " 'typic',\n",
       " 'everywher',\n",
       " 'dirt',\n",
       " 'floor',\n",
       " 'window',\n",
       " 'shirt',\n",
       " 'sometim',\n",
       " 'mouth',\n",
       " 'dream',\n",
       " 'without',\n",
       " 'chore',\n",
       " 'joy',\n",
       " 'tv',\n",
       " 'exist',\n",
       " 'hail',\n",
       " 'mist',\n",
       " 'becom',\n",
       " 'aaooooright',\n",
       " 'leav',\n",
       " 'hous',\n",
       " 'interview',\n",
       " 'boy',\n",
       " 'annonc',\n",
       " 'arrang',\n",
       " 'keep',\n",
       " 'safe',\n",
       " 'becaus',\n",
       " 'envi',\n",
       " 'everyon',\n",
       " 'parent',\n",
       " 'hand',\n",
       " 'excit',\n",
       " 'each',\n",
       " 'spend',\n",
       " 'bootydeli',\n",
       " 'f',\n",
       " 'bangbab',\n",
       " 'order',\n",
       " 'should',\n",
       " 'content',\n",
       " 'goto',\n",
       " 'bangb',\n",
       " 'internet',\n",
       " 'menu',\n",
       " 'cultur',\n",
       " 'modul',\n",
       " 'snumber',\n",
       " 'avoid',\n",
       " 'missunderstd',\n",
       " 'wit',\n",
       " 'belov',\n",
       " 'escap',\n",
       " 'fanci',\n",
       " 'bridg',\n",
       " 'lager',\n",
       " 'complet',\n",
       " 'form',\n",
       " 'clark',\n",
       " 'also',\n",
       " 'utter',\n",
       " 'wast',\n",
       " 'axi',\n",
       " 'bank',\n",
       " 'hmmm',\n",
       " 'hop',\n",
       " 'muz',\n",
       " 'discuss',\n",
       " 'liao',\n",
       " 'bloodi',\n",
       " 'hell',\n",
       " 'cant',\n",
       " 'believ',\n",
       " 'surnam',\n",
       " 'mr',\n",
       " 'ill',\n",
       " 'clue',\n",
       " 'spanish',\n",
       " 'bath',\n",
       " 'carlo',\n",
       " 'mall',\n",
       " 'stay',\n",
       " 'til',\n",
       " 'smoke',\n",
       " 'moneysymb',\n",
       " 'worth',\n",
       " 'doesnt',\n",
       " 'log',\n",
       " 'spoke',\n",
       " 'maneesha',\n",
       " 'satisfi',\n",
       " 'experi',\n",
       " 'toll',\n",
       " 'lift',\n",
       " 'especi',\n",
       " 'approach',\n",
       " 'studi',\n",
       " 'grnumber',\n",
       " 'trust',\n",
       " 'guy',\n",
       " 'bye',\n",
       " 'handsom',\n",
       " 'toward',\n",
       " 'mummi',\n",
       " 'boytoy',\n",
       " 'awesom',\n",
       " 'minut',\n",
       " 'freephon',\n",
       " 'xma',\n",
       " 'radio',\n",
       " 'ju',\n",
       " 'si',\n",
       " 'uniqu',\n",
       " 'august',\n",
       " 'areyouuniqu',\n",
       " 'leagu',\n",
       " 'touch',\n",
       " 'deal',\n",
       " 'cours',\n",
       " 'itself',\n",
       " 'howev',\n",
       " 'suggest',\n",
       " 'abl',\n",
       " 'everi',\n",
       " 'stool',\n",
       " 'settl',\n",
       " 'wishin',\n",
       " 'mrng',\n",
       " 'hav',\n",
       " 'stori',\n",
       " 'hamster',\n",
       " 'dead',\n",
       " 'tmr',\n",
       " ...]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "until\n",
      "jurong\n",
      "point\n",
      "crazi\n",
      "avail\n",
      "onli\n",
      "in\n",
      "bugi\n",
      "n\n",
      "great\n",
      "world\n",
      "la\n",
      "e\n",
      "buffet\n",
      "cine\n",
      "there\n",
      "got\n",
      "amor\n",
      "wat\n"
     ]
    }
   ],
   "source": [
    "# define a find_features function\n",
    "\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "#An example\n",
    "features = find_features(processed[0])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go until jurong point crazi avail onli in bugi n great world la e buffet cine there got amor wat'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find features for all messages\n",
    "messages = list(zip(processed, Y))\n",
    "\n",
    "# define a seed for responsibility\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "# call find_features function for each SMS mesages\n",
    "featuresets = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing data sets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 4179\n",
      "Testing: 1393\n"
     ]
    }
   ],
   "source": [
    "print('Training: {}'.format(len(training)))\n",
    "print('Testing: {}'.format(len(testing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Scikit-Learn Classifier with NLTK</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train \n",
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Logistic Regression', 'SGD Classifier', 'Naive Bayes', 'SVM Linear']\n",
    "\n",
    "classifiers = {\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel='linear')\n",
    "}\n",
    "\n",
    "models = list(zip(names, classifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors: Accuracy: 98.42067480258436\n",
      "Decision Tree: Accuracy: 97.20028715003589\n",
      "Random Forest: Accuracy: 98.56424982053123\n",
      "Logistic Regression: Accuracy: 98.20531227566404\n",
      "SGD Classifier: Accuracy: 93.46733668341709\n",
      "Naive Bayes: Accuracy: 97.84637473079684\n",
      "SVM Linear: Accuracy: 98.1335247666906\n"
     ]
    }
   ],
   "source": [
    "# wrap models in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing) * 100\n",
    "    print('{}: Accuracy: {}'.format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Method Accuracy: 98.56424982053123\n"
     ]
    }
   ],
   "source": [
    "# ensemble method- Voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define models to train \n",
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Logistic Regression', 'SGD Classifier', 'Naive Bayes', 'SVM Linear']\n",
    "\n",
    "classifiers = {\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel='linear')\n",
    "}\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_ensemble, testing) * 100\n",
    "print('Ensemble Method Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class label prediction for testing set\n",
    "text_features, labels = list(zip(*testing))\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1203\n",
      "           1       0.99      0.91      0.95       190\n",
      "\n",
      "    accuracy                           0.99      1393\n",
      "   macro avg       0.99      0.95      0.97      1393\n",
      "weighted avg       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>18</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1201    2\n",
       "       spam        18  172"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a cofusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "    columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
